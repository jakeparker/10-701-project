# 10-701 Project

## Project Proposal Rubric
*  [NIPS format](https://nips.cc/Conferences/2017/PaperInformation/StyleFiles) (one page excluding references, no abstract needed)  
*  A well-stated problem that you plan to explore  
*  The motivation behind this problem  
*  Relevant prior work (or similar work), if any  
*  Your proposed idea/method/approach  
*  How do you plan to implement your approach (e.g. for deep learning, tensorflow/pytorch/...?)  
*  A timeline of this project (just give an estimate), and what dataset you plan to use  


## Potential Project Ideas

---

*  Compound Figure Separation  
   _(deep learning)_ _(convolutional neural networks)_  
   *  Compound Figure Separation using CNN [(abstract)](https://arxiv.org/abs/1703.05105) [(paper)](http://ceur-ws.org/Vol-1391/25-CR.pdf)   
   *  Crowdsourcing for Compound Figure Annotation [(paper)](https://lhncbc.nlm.nih.gov/system/files/pub9437.pdf) 
   *  Automatic Seperation of Compound Figures [(paper)](http://www-itec.uni-klu.ac.at/bib/files/fig-separation-mtap.pdf)   
   *  ImageCLEFmed 2016 [(abstract)](http://www.imageclef.org/2016/medical)  
   *  ImageCLEFmed 2015 [(abstract)](http://www.imageclef.org/2015/medical)  
   *  FigureQA [(abstract)](https://arxiv.org/abs/1710.07300) [(paper)](https://arxiv.org/pdf/1710.07300.pdf) [(website)](https://datasets.maluuba.com/FigureQA)  
   *  Scalable Algorithms for Scholarly Figure Mining [(slides)](https://www.ifis.uni-luebeck.de/~groppe/sbd/resources/2016/slides/SBD16-s1-t2.pdf)  
 
 
*  Transfer Learning   
   *  Overcoming Data Scarcity with Transfer Learning [(abstract)](https://arxiv.org/abs/1711.05099)  


*  Causality  
   _(experimental design)_  _(counterfactual prediction)_ _(deep learning)_  
   *  DeepIV: Counterfactual Prediction using Instrument Variables [(abstract)](http://proceedings.mlr.press/v70/hartford17a.html) [(github)](https://github.com/jhartford/DeepIV)  
   *  A Minimax Surrogate Loss Approach to Causal Inference [(paper)](http://web.mit.edu/stgoh/www/mypage/causalwebsite.pdf)  


*  Information Retrieval  
   _(probabilistic graphical models)_  
   *  Credibility Analysis [(abstract)](https://arxiv.org/abs/1707.08309)  
   *  Latent Credibility Analysis [(abstract)](https://experts.illinois.edu/en/publications/latent-credibility-analysis)  
   *  Information Credibility [(abstract)](https://link.springer.com/chapter/10.1007/978-3-319-29175-8_12)  
   *  Dissertation [(abstract)](https://smartech.gatech.edu/handle/1853/55646) [(paper)](https://smartech.gatech.edu/bitstream/handle/1853/55646/ZOU-DISSERTATION-2016.pdf?sequence=1&isAllowed=y)  


*  Infinitely differentiable loss function [(Monte Carlo Estimator)](https://en.wikipedia.org/wiki/Monte_Carlo_method)  
   _(optimization)_  _(deep learning)_ _(reinforcement learning)_  
   *  DiCE: The Infinitely Differentiable Monte Carlo Estimator [(abstract)](https://arxiv.org/abs/1802.05098)  


*  Automatic generation of [surrogate loss functions](https://stats.stackexchange.com/a/267509/147296) given non-differentiable loss functions ([ACKTR, A2C](https://blog.openai.com/baselines-acktr-a2c/), [PPO](https://blog.openai.com/openai-baselines-ppo/))  
   _(optimization)_  
   *  Surrogate Loss Functions [(blog post)](http://fa.bianp.net/blog/2014/surrogate-loss-functions-in-machine-learning/)  
   *  ML for non-differentiable Loss Functions [(blog post)](http://khanhxnguyen.com/machine-learning-non-differentiable-loss-functions/)  
   *  Comparing Loss Functions by Risk [(paper)](http://www.isa.uni-stuttgart.de/Steinwart/Publikationen/2007/Steinwart07a.pdf)  
   *  Surrogate Loss Functions and Æ’-divergences [(paper)](https://projecteuclid.org/download/pdfview_1/euclid.aos/1236693153)  


---
